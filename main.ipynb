{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline,DDIMScheduler\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import abc\n",
    "import ptp_utils as ptp_utils\n",
    "import seq_aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOW_RESOURCE = False \n",
    "NUM_DIFFUSION_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(\"sd-legacy/stable-diffusion-v1-5\", torch_dtype=torch.bfloat16).to(device)\n",
    "from diffusers import DDIMScheduler\n",
    "ldm_stable.scheduler = DDIMScheduler.from_config(ldm_stable.scheduler.config)\n",
    "tokenizer = ldm_stable.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Prompt-to-Prompt Attnetion Controllers\n",
    "Our main logic is implemented in the `forward` call in an `AttentionControl` object.\n",
    "The forward is called in each attention layer of the diffusion model and it can modify the input attnetion weights `attn`.\n",
    "\n",
    "`is_cross`, `place_in_unet in (\"down\", \"mid\", \"up\")`, `AttentionControl.cur_step` help us track the exact attention layer and timestamp during the diffusion iference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalBlend:\n",
    "\n",
    "    def __call__(self, x_t, attention_store):\n",
    "        k = 1\n",
    "        maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
    "        maps = [item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS) for item in maps]\n",
    "        maps = torch.cat(maps, dim=1)\n",
    "        maps = (maps * self.alpha_layers).sum(-1).mean(1)\n",
    "        mask = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 +1), (1, 1), padding=(k, k))\n",
    "        mask = nnf.interpolate(mask, size=(x_t.shape[2:]))\n",
    "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
    "        mask = mask.gt(self.threshold)\n",
    "        mask = (mask[:1] + mask[1:]).float()\n",
    "        x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
    "        return x_t\n",
    "       \n",
    "    def __init__(self, prompts: List[str], words: [List[List[str]]], threshold=.3):\n",
    "        alpha_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
    "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
    "            if type(words_) is str:\n",
    "                words_ = [words_]\n",
    "            for word in words_:\n",
    "                ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
    "                alpha_layers[i, :, :, :, :, ind] = 1\n",
    "        self.alpha_layers = alpha_layers.to(device)\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "class AttentionControl(abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "    \n",
    "    def between_steps(self):\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def num_uncond_att_layers(self):\n",
    "        return self.num_att_layers if LOW_RESOURCE else 0\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
    "            if LOW_RESOURCE:\n",
    "                attn = self.forward(attn, is_cross, place_in_unet)\n",
    "            else:\n",
    "                # 修正: 避免 in-place 操作，使用 clone() 和 cat()\n",
    "                h = attn.shape[0]\n",
    "                attn_uncond = attn[:h // 2]\n",
    "                attn_cond = self.forward(attn[h // 2:].clone(), is_cross, place_in_unet)\n",
    "                attn = torch.cat([attn_uncond, attn_cond], dim=0)\n",
    "               \n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "            self.between_steps()\n",
    "        return attn\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.cur_att_layer = 0\n",
    "\n",
    "class EmptyControl(AttentionControl):\n",
    "    \n",
    "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
    "        return attn\n",
    " \n",
    "class AttentionStore(AttentionControl):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_empty_store():\n",
    "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
    "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
    "\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
    "        if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
    "            self.step_store[key].append(attn)\n",
    "        return attn\n",
    "\n",
    "    def between_steps(self):\n",
    "        if len(self.attention_store) == 0:\n",
    "            self.attention_store = self.step_store\n",
    "        else:\n",
    "            for key in self.attention_store:\n",
    "                for i in range(len(self.attention_store[key])):\n",
    "                    self.attention_store[key][i] += self.step_store[key][i]\n",
    "        self.step_store = self.get_empty_store()\n",
    "\n",
    "    def get_average_attention(self):\n",
    "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
    "        return average_attention\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        super(AttentionStore, self).reset()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionStore, self).__init__()\n",
    "        self.step_store = self.get_empty_store()\n",
    "        self.attention_store = {}\n",
    "    \n",
    "        \n",
    "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
    "    \n",
    "    def step_callback(self, x_t):\n",
    "        if self.local_blend is not None:\n",
    "            x_t = self.local_blend(x_t, self.attention_store)\n",
    "        return x_t\n",
    "        \n",
    "    def replace_self_attention(self, attn_base, att_replace):\n",
    "        if att_replace.shape[2] <= 16 ** 2:\n",
    "            return attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
    "        else:\n",
    "            return att_replace\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
    "        if is_cross or (self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]):\n",
    "            h = attn.shape[0] // (self.batch_size)\n",
    "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
    "            attn_base, attn_repalce = attn[0], attn[1:]\n",
    "            if is_cross:\n",
    "                # 轉換 alpha_words 到與 attn 相同的 dtype\n",
    "                alpha_words = self.cross_replace_alpha[self.cur_step].to(attn.dtype)\n",
    "                attn_repalce_new = self.replace_cross_attention(attn_base, attn_repalce) * alpha_words + (1 - alpha_words) * attn_repalce\n",
    "                attn = torch.cat([attn_base.unsqueeze(0), attn_repalce_new], dim=0)\n",
    "            else:\n",
    "                attn = torch.cat([attn_base.unsqueeze(0), self.replace_self_attention(attn_base, attn_repalce)], dim=0)\n",
    "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
    "        return attn\n",
    "    \n",
    "    def __init__(self, prompts, num_steps: int,\n",
    "                 cross_replace_steps: Union[float, Tuple[float, float], Dict[str, Tuple[float, float]]],\n",
    "                 self_replace_steps: Union[float, Tuple[float, float]],\n",
    "                 local_blend: Optional[LocalBlend]):\n",
    "        super(AttentionControlEdit, self).__init__()\n",
    "        self.batch_size = len(prompts)\n",
    "        self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(prompts, num_steps, cross_replace_steps, tokenizer).to(device)\n",
    "        if type(self_replace_steps) is float:\n",
    "            self_replace_steps = 0, self_replace_steps\n",
    "        self.num_self_replace = int(num_steps * self_replace_steps[0]), int(num_steps * self_replace_steps[1])\n",
    "        self.local_blend = local_blend\n",
    "\n",
    "class AttentionRefine(AttentionControlEdit):\n",
    "\n",
    "    def replace_cross_attention(self, attn_base, att_replace):\n",
    "        attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
    "        # 轉換 alphas 到與 attn 相同的 dtype\n",
    "        alphas = self.alphas.to(attn_base.dtype)\n",
    "        attn_replace = attn_base_replace * alphas + att_replace * (1 - alphas)\n",
    "        return attn_replace\n",
    "\n",
    "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
    "                 local_blend: Optional[LocalBlend] = None):\n",
    "        super(AttentionRefine, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
    "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
    "        self.mapper, alphas = self.mapper.to(device), alphas.to(device)\n",
    "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實驗開始前的設定修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_config import TrainConfig,ExperimentConfig\n",
    "import glob\n",
    "import os\n",
    "cfg = ExperimentConfig(\n",
    "    exp_dir=\"20251203_014400\",\n",
    "    # 這邊是實驗的名稱會存在experiments/這個資料夾底下 new代表會依照實驗時間自動建立子資料夾，不是new的話就會直接存在experiments/{exp_dir}底下\n",
    "    base_dir=\"experiments/\",\n",
    "    source_image_path=\"/home/ksp0108/workspace/HW/video_gen/prompt-to-prompt/dataset/test_1201(pair_data)/B_05.png\",\n",
    "    #BEFORE圖片路徑\n",
    "    target_image_path=\"/home/ksp0108/workspace/HW/video_gen/prompt-to-prompt/dataset/test_1201(pair_data)/A_05.png\",\n",
    "    #AFTER圖片路徑\n",
    "    test_image_pattern= \"/home/ksp0108/workspace/HW/video_gen/prompt-to-prompt/dataset/test_1130(single_data)/*.png\",\n",
    "    #這邊是glob.glob的pattern會把符合的圖片都拿來做測試\n",
    "    low_resource= False,\n",
    "    #不知道什麼意思就不要動它，原本P2P留下的\n",
    "    num_diffusion_steps= 50,\n",
    "    # 這邊是sd的diffusion steps數量，預設就50不要動\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 以下為實驗可調整的參數\n",
    "    option_guidance_scale= [1, 3.5, 7.5],\n",
    "    # guidance_scale 可以 1 3.5 7.5去比較效果\n",
    "    option_cross_replace_step= [[0.2,1.0]], #等價於論文的 tau =0.8\n",
    "    #這邊每一個serach instance是一個list第一項是開始交換的tau比例第二項是結束交換的tau比例\n",
    "    #論文中的if tau(0.7)>t -> do differential attention control，T是從1開始倒數到0\n",
    "    #但我們這邊時做事候，是從0開始數到1所以要做交換的tau比例要做反轉，例如論文中tau=0.7代表我們這裡t從0.3,1.0要做交換\n",
    "    option_encoded_emb= [False],\n",
    "    # sd 1.5處理文字的流程如下: text->tokenizer->embedding->text encoder\n",
    "    # 如果是encoded_emb=False，代表我們訓練的是進入text encoder前的embedding向量(這個是paper主要實驗的設定包含text-inversion這篇也是這樣做)\n",
    "    # 如果是encoded_emb=True，代表我們訓練的是進入text encoder後的embedding向量\n",
    "    option_self_replace_step= [0.],\n",
    "    # 這邊論文對self的交換沒有寫得很詳細跳過這個部分\n",
    "    coarse_description= \"a watercolor painting\",\n",
    "    #coarse_description是用來控制風格的描述詞初始化用的\n",
    "\n",
    "\n",
    "    lr= 0.001,\n",
    "    optimizer_cls= torch.optim.AdamW,\n",
    "    num_epochs= 40,\n",
    "    save_interval= 1,\n",
    "    beta_weighting= True,\n",
    "    test_epochs= [0, 5, 10, 20, 30, 39]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搜尋的參數組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all TrainConfig objects from ExperimentConfig\n",
    "train_configs = cfg.generate_train_configs()\n",
    "\n",
    "# Filter out already completed experiments\n",
    "filtered_configs = []\n",
    "for train_cfg in train_configs:\n",
    "    save_image_dir = train_cfg.get_save_dir()\n",
    "    if os.path.exists(os.path.join(save_image_dir, \"final.pt\")):\n",
    "        print(f\"Skipping {save_image_dir} since final.pt exists\")\n",
    "        continue\n",
    "    filtered_configs.append(train_cfg)\n",
    "train_configs = filtered_configs\n",
    "\n",
    "print(f\"Running {len(train_configs)} experiments\")\n",
    "if train_configs:\n",
    "    exp_dir = train_configs[0].exp_dir\n",
    "    print(f\"Experiment directory: {exp_dir}\")\n",
    "    # Save experiment config to experiment directory\n",
    "    cfg_save_path = os.path.join(exp_dir, \"config.json\")\n",
    "    cfg.save(cfg_save_path)\n",
    "    print(f\"Config saved to {cfg_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 對訓練影像作前處理(DDIM inversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images using paths from config\n",
    "from inversion import invert\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "source_image = Image.open(cfg.source_image_path).convert(\"RGB\").resize((512, 512))\n",
    "target_image = Image.open(cfg.target_image_path).convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "# Convert to latents\n",
    "before_latent = ptp_utils.image2latent(ldm_stable.vae, np.array(source_image).reshape(1, 512, 512, 3))\n",
    "after_latent = ptp_utils.image2latent(ldm_stable.vae, np.array(target_image).reshape(1, 512, 512, 3))\n",
    "\n",
    "# Do inversion\n",
    "noised_before_latent = invert(\n",
    "    ldm_stable,\n",
    "    before_latent,\n",
    "    prompt=\"\",\n",
    "    guidance_scale=1,\n",
    "    num_inference_steps=cfg.num_diffusion_steps,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"Images loaded and inverted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_cfg in train_configs:\n",
    "    save_image_dir = train_cfg.get_save_dir()\n",
    "    os.makedirs(save_image_dir, exist_ok=True)\n",
    "    \n",
    "    # Save TrainConfig to experiment folder\n",
    "    train_cfg_path = os.path.join(save_image_dir, \"train_config.json\")\n",
    "    train_cfg.save(train_cfg_path)\n",
    "    print(f\"TrainConfig saved to {train_cfg_path}\")\n",
    "    \n",
    "    # Create attention controller\n",
    "    controller = AttentionRefine(\n",
    "        prompts=[\"\", train_cfg.coarse_description],\n",
    "        num_steps=train_cfg.num_diffusion_steps,\n",
    "        cross_replace_steps=train_cfg.cross_replace_step,\n",
    "        self_replace_steps=train_cfg.self_replace_step,\n",
    "    )\n",
    "    \n",
    "    # Setup for training\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    ldm_stable.vae.requires_grad_(False)\n",
    "    ldm_stable.unet.requires_grad_(False)\n",
    "    ldm_stable.text_encoder.requires_grad_(False)\n",
    "    ldm_stable.scheduler = DDIMScheduler.from_pretrained(\"sd-legacy/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
    "    ldm_stable.scheduler.set_timesteps(train_cfg.num_diffusion_steps)\n",
    "    \n",
    "    # Select training function based on config\n",
    "    train_fn = ptp_utils.train_text_embedding_ldm_stable if train_cfg.encoded_emb else ptp_utils.train_text_embedding_ldm_stable_with_out_encode\n",
    "    \n",
    "    # Train\n",
    "    learned_emb = train_fn(\n",
    "        ldm_stable,\n",
    "        train_cfg.coarse_description,\n",
    "        controller,\n",
    "        noised_before_latent,\n",
    "        after_latent,\n",
    "        num_steps=train_cfg.num_diffusion_steps,\n",
    "        epoch=train_cfg.num_epochs,\n",
    "        guidance_scale=train_cfg.guidance_scale,\n",
    "        lr=train_cfg.lr,\n",
    "        optimizer_cls=train_cfg.optimizer_cls,\n",
    "        save_interval=train_cfg.save_interval,\n",
    "        save_image_dir=save_image_dir,\n",
    "        beta_weighting=train_cfg.beta_weighting\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取訓練後的模型，和配置進行測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_config import get_or_create_exp_dir,ExperimentConfig\n",
    "import os\n",
    "target_exp_dir = get_or_create_exp_dir(\n",
    "    base_dir=\"experiments/\",\n",
    "    exp_dir=None #這邊可以指定要讀取哪一個實驗的資料夾名稱(如:20251203_005726)，如果為None則會挑最新的一次\n",
    ")#experiments/20251203_005726\n",
    "target_exp_id=target_exp_dir.split(\"/\")[-1]#20251203_005726\n",
    "cfg = ExperimentConfig().load(os.path.join(target_exp_dir, \"config.json\"))\n",
    "test_configs=cfg.generate_train_configs()\n",
    "all_test_image = glob.glob(cfg.test_image_pattern)\n",
    "print(f\"Testing experiments in: {test_configs[0].exp_dir if test_configs else 'N/A'}\")\n",
    "print(f\"Test images: {len(all_test_image)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import ImageGrid\n",
    "# Define column labels based on test epochs\n",
    "col_labels = [\"Source\", \"Epoch0\"] + [f\"Epoch{e}\" for e in cfg.test_epochs[1:]]\n",
    "\n",
    "# Run inference on test images\n",
    "for test_cfg in test_configs:\n",
    "    exp_dir = test_cfg.exp_dir\n",
    "    \n",
    "    # Create ImageGrid with labels\n",
    "    row_labels = [os.path.basename(p).replace('.jpg', '') for p in all_test_image]\n",
    "    image_grid = ImageGrid(row_labels=row_labels, col_labels=col_labels)\n",
    "    \n",
    "    for image_path in all_test_image:\n",
    "        before_image = Image.open(image_path).convert(\"RGB\").resize((512, 512))\n",
    "        before_latent = ptp_utils.image2latent(ldm_stable.vae, np.array(before_image).reshape(1, 512, 512, 3))\n",
    "        \n",
    "        noised_before_latent = invert(\n",
    "            ldm_stable,\n",
    "            before_latent,\n",
    "            prompt=\"\",\n",
    "            guidance_scale=1,\n",
    "            num_inference_steps=test_cfg.num_diffusion_steps,\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        for e in test_cfg.test_epochs:\n",
    "            learned_emb_path = os.path.join(exp_dir, test_cfg.exp_name, f\"epoch_{e}.pt\")\n",
    "            learned_emb = torch.load(learned_emb_path).to(device)\n",
    "            \n",
    "            # Create controller\n",
    "            controller = AttentionRefine(\n",
    "                prompts=[\"\", test_cfg.coarse_description],\n",
    "                num_steps=test_cfg.num_diffusion_steps,\n",
    "                cross_replace_steps=test_cfg.cross_replace_step,\n",
    "                self_replace_steps=0.0,\n",
    "            )\n",
    "            \n",
    "            images, x_t = ptp_utils.text2image_ldm_stable_with_learned_embedding(\n",
    "                ldm_stable,\n",
    "                learned_emb=learned_emb,\n",
    "                controller=controller,\n",
    "                latent=noised_before_latent,\n",
    "                num_inference_steps=test_cfg.num_diffusion_steps,\n",
    "                guidance_scale=test_cfg.guidance_scale,\n",
    "                low_resource=test_cfg.low_resource\n",
    "            )\n",
    "            \n",
    "            if e == test_cfg.test_epochs[0]:\n",
    "                # Add source image and first epoch result\n",
    "                image_grid.add_image([images[0], images[1]])\n",
    "            else:\n",
    "                image_grid.add_image(images[1])\n",
    "    \n",
    "    # Save with row and column labels\n",
    "    save_path = os.path.join(exp_dir, test_cfg.exp_name, \"test_image1.png\")\n",
    "    image_grid.save(save_path, num_rows=len(all_test_image))\n",
    "    print(f\"Saved images to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_configs = cfg.generate_train_configs()\n",
    "\n",
    "# Copy results to comparison folder within the experiment directory\n",
    "if result_configs:\n",
    "    exp_dir = result_configs[0].exp_dir\n",
    "    comparison_dir = os.path.join(exp_dir, \"all_option_combinations\")\n",
    "    os.makedirs(comparison_dir, exist_ok=True)\n",
    "    \n",
    "    for result_cfg in result_configs:\n",
    "        test_image_path = os.path.join(exp_dir, result_cfg.exp_name, \"test_image1.png\")\n",
    "        safe_name = result_cfg.exp_name.replace('(', '').replace(')', '').replace(',', '_').replace('=', '_')\n",
    "        dest_path = os.path.join(comparison_dir, f\"{safe_name}_test_image1.png\")\n",
    "        \n",
    "        if os.path.exists(test_image_path):\n",
    "            os.system(f\"cp '{test_image_path}' '{dest_path}'\")\n",
    "    \n",
    "    print(f\"Results copied to {comparison_dir}/\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "video_synthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
